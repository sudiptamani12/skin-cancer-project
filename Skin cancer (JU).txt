# Import necessary libraries
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
import numpy as np
import cv2
import os
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
from sklearn.model_selection import train_test_split
import xgboost as xgb
from google.colab import drive
import time

# Mount Google Drive
drive.mount('/content/gdrive')

# Unzip Dataset (ensure path is correct)
!unzip "/content/gdrive/MyDrive/SkinCancer/Dataset.zip" > /dev/null

# Load dataset from the unzipped folder (replace with actual dataset paths)
def load_images(image_folder, target_size=(224, 224)):
    images = []
    labels = []
    label_map = {'benign': 0, 'malignant': 1, 'precancerous': 2}  # Update according to your labels
    for label in label_map:
        folder_path = os.path.join(image_folder, label)
        for img_name in os.listdir(folder_path):
            img_path = os.path.join(folder_path, img_name)
            img = cv2.imread(img_path)
            img = cv2.resize(img, target_size)
            img = img / 255.0  # Normalize
            images.append(img)
            labels.append(label_map[label])
    return np.array(images), np.array(labels)

# Count number of images in Train and Test directories
def count_images(image_folder):
    count = {}
    for label in os.listdir(image_folder):
        label_folder = os.path.join(image_folder, label)
        count[label] = len(os.listdir(label_folder))
    return count

# Define directories for your dataset
train_image_folder = "/content/gdrive/MyDrive/SkinCancer/Dataset/train"
test_image_folder = "/content/gdrive/MyDrive/SkinCancer/Dataset/test"

# Load dataset
train_images, train_labels = load_images(train_image_folder)
test_images, test_labels = load_images(test_image_folder)

# Count the number of images in train and test directories
train_counts = count_images(train_image_folder)
test_counts = count_images(test_image_folder)

print(f"Training data class distribution: {train_counts}")
print(f"Testing data class distribution: {test_counts}")

# Data Visualization: Distribution of classes in the training dataset
def plot_class_distribution(counts, title='Class Distribution'):
    labels = list(counts.keys())
    values = list(counts.values())
    plt.figure(figsize=(8, 6))
    plt.bar(labels, values, color='skyblue')
    plt.xlabel('Classes')
    plt.ylabel('Number of Images')
    plt.title(title)
    plt.show()

# Visualize class distribution in the training dataset
plot_class_distribution(train_counts, 'Training Dataset Class Distribution')

# Data Augmentation
datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode="nearest"
)

datagen.fit(train_images)

# Split training data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)

# Model Building: CNN + ViT + XGBoost Hybrid Model
def create_cnn_vit_model(input_shape=(224, 224, 3)):
    # CNN component
    cnn_input = layers.Input(shape=input_shape)
    x = layers.Conv2D(32, (3, 3), activation='relu')(cnn_input)
    x = layers.MaxPooling2D((2, 2))(x)
    x = layers.Conv2D(64, (3, 3), activation='relu')(x)
    x = layers.MaxPooling2D((2, 2))(x)
    x = layers.Flatten()(x)

    # ViT component (Vision Transformer)
    vit_input = layers.Input(shape=input_shape)
    vit_x = layers.Reshape((-1, 224))(vit_input)  # Reshape for ViT input
    vit_x = layers.MultiHeadAttention(num_heads=4, key_dim=64)(vit_x, vit_x)
    vit_x = layers.GlobalAveragePooling1D()(vit_x)

    # Merging CNN and ViT
    combined = layers.concatenate([x, vit_x])

    # Final Classification Layer
    output = layers.Dense(3, activation='softmax')(combined)
    
    model = models.Model(inputs=[cnn_input, vit_input], outputs=output)
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    
    return model

# Instantiate the model
cnn_vit_model = create_cnn_vit_model()

# Model Architecture summary
cnn_vit_model.summary()

# Training: Measure elapsed time
start_time = time.time()

history = cnn_vit_model.fit(
    [X_train, X_train], y_train,  # Using the same X_train for both inputs (CNN and ViT)
    validation_data=([X_val, X_val], y_val),
    epochs=10,
    batch_size=32
)

elapsed_time = time.time() - start_time
print(f"Training time: {elapsed_time:.2f} seconds")

# Plotting Training and Validation Accuracy & Loss
def plot_accuracy_loss(history):
    # Accuracy Plot
    plt.figure(figsize=(10, 6))
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

    # Loss Plot
    plt.figure(figsize=(10, 6))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Visualize accuracy and loss curves
plot_accuracy_loss(history)

# Evaluate the model
loss, accuracy = cnn_vit_model.evaluate([test_images, test_images], test_labels)
print(f"Test Accuracy: {accuracy:.4f}")

# Generate Confusion Matrix
y_pred = np.argmax(cnn_vit_model.predict([test_images, test_images]), axis=1)
conf_matrix = confusion_matrix(test_labels, y_pred)

# Display Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Benign', 'Malignant', 'Precancerous'], yticklabels=['Benign', 'Malignant', 'Precancerous'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

# Classification Report
print(classification_report(test_labels, y_pred, target_names=['Benign', 'Malignant', 'Precancerous']))

# XGBoost for post-processing (optional)
xgboost_model = xgb.XGBClassifier(objective="multi:softmax", num_class=3)
xgboost_model.fit(X_train.reshape(X_train.shape[0], -1), y_train)
y_pred_xgb = xgboost_model.predict(X_test.reshape(X_test.shape[0], -1))

# Evaluate XGBoost Model
print(f"XGBoost Accuracy: {accuracy_score(test_labels, y_pred_xgb)}")
